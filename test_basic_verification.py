def visualize_attention(text="Hello world, I am a student."):
    """
    å®Œæ•´çš„æ³¨æ„åŠ›å¯è§†åŒ–æµç¨‹ï¼šæ¨¡å‹æ¨ç† -> æƒé‡èšåˆ -> HTMLç”Ÿæˆ
    """
    from modelscope import AutoModel, AutoTokenizer
    import torch
    
    # åŠ è½½æ¨¡å‹
    model_name = "Qwen/Qwen2-0.5B-Instruct"
    model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    
    # è·å–tokenizerä¿¡æ¯ç”¨äºæ˜¾ç¤º
    tokenizer_info = {
        'model_name': model_name,
        'tokenizer_type': type(tokenizer).__name__,
        'vocab_size': tokenizer.vocab_size if hasattr(tokenizer, 'vocab_size') else 'Unknown',
        'special_tokens_count': len(tokenizer.special_tokens_map) if hasattr(tokenizer, 'special_tokens_map') else 0
    }
    
    # æ³¨æ„åŠ›æå–
    inputs = tokenizer(text, return_tensors="pt")
    outputs = model(**inputs, output_attentions=True)
    
    # æƒé‡èšåˆ
    attention = outputs.attentions[-1]  # æœ€åä¸€å±‚
    averaged = attention.mean(dim=1)    # å¹³å‡å¤šå¤´
    weights = averaged.sum(dim=-2)      # åˆ—æ±‚å’Œ
    weights = weights.squeeze().tolist()
    
    # è·å–tokens (ä¿®å¤ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜)
    input_ids = inputs["input_ids"][0]
    tokens = []
    
    # é€ä¸ªå¤„ç†tokenï¼Œç¡®ä¿ä¸­æ–‡æ­£ç¡®æ˜¾ç¤º
    for i, token_id in enumerate(input_ids):
        # è·å–å•ä¸ªtokenå¯¹åº”çš„æ–‡æœ¬
        single_token_text = tokenizer.decode([token_id])
        tokens.append(single_token_text)
    
    # è°ƒè¯•ä¿¡æ¯å·²å»é™¤ï¼Œé¿å…ç¼–ç é—®é¢˜
    
    # å½’ä¸€åŒ–æƒé‡åˆ°[0,1]
    min_w, max_w = min(weights), max(weights)
    normalized_weights = [(w - min_w) / (max_w - min_w) for w in weights]
    
    # ç”ŸæˆHTML
    html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Attention Visualization</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }}
        .token {{ padding: 2px 4px; margin: 1px; border-radius: 3px; }}
        .info {{ margin: 20px 0; color: #666; }}
        .tokenizer-info {{ 
            background: #f8f9fa; 
            border: 1px solid #dee2e6; 
            border-radius: 8px; 
            padding: 15px; 
            margin: 20px 0; 
            font-size: 14px;
        }}
        .tokenizer-info h3 {{ margin-top: 0; color: #495057; }}
        .warning {{ 
            background: #fff3cd; 
            border: 1px solid #ffeaa7; 
            border-radius: 6px; 
            padding: 12px; 
            margin: 15px 0; 
            color: #856404;
        }}
        .tech-details {{ color: #6c757d; font-family: monospace; }}
    </style>
</head>
<body>
    <h1>æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–</h1>
    
    <div class="tokenizer-info">
        <h3>ğŸ”§ Tokenizer æŠ€æœ¯ä¿¡æ¯</h3>
        <div class="tech-details">
            <strong>æ¨¡å‹:</strong> {tokenizer_info['model_name']}<br>
            <strong>Tokenizerç±»å‹:</strong> {tokenizer_info['tokenizer_type']}<br>
            <strong>è¯æ±‡è¡¨å¤§å°:</strong> {tokenizer_info['vocab_size']:,} tokens<br>
            <strong>ç‰¹æ®Šæ ‡è®°æ•°é‡:</strong> {tokenizer_info['special_tokens_count']}
        </div>
    </div>
    
    <div class="warning">
        <strong>âš ï¸ é‡è¦æé†’:</strong><br>
        â€¢ ä¸åŒæ¨¡å‹çš„tokenizerä¼šäº§ç”Ÿä¸åŒçš„åˆ†è¯ç»“æœ<br>
        â€¢ Qwenç³»åˆ—å¯¹ä¸­æ–‡åˆ†è¯å‹å¥½ï¼Œä½†åˆ‡æ¢æ¨¡å‹ä¼šæ”¹å˜å¯è§†åŒ–ç»“æœ<br>
        â€¢ æ³¨æ„åŠ›æƒé‡åæ˜ çš„æ˜¯tokençº§åˆ«çš„å…³ç³»ï¼Œä¸æ˜¯è¯çº§åˆ«<br>
        â€¢ åŒä¸€å¥è¯ç”¨ä¸åŒtokenizerå¯èƒ½äº§ç”Ÿå®Œå…¨ä¸åŒçš„æƒé‡åˆ†å¸ƒ
    </div>
    
    <div class="info">
        <strong>è¾“å…¥æ–‡æœ¬:</strong> "{text}"<br>
        <strong>Tokenæ•°é‡:</strong> {len(tokens)} ä¸ª
    </div>
    
    <div class="visualization">
"""
    
    # ä¸ºæ¯ä¸ªtokenç”Ÿæˆé«˜äº®HTML
    for token, weight in zip(tokens, normalized_weights):
        # ä½¿ç”¨çº¢è‰²æ·±æµ…è¡¨ç¤ºæƒé‡å¼ºåº¦
        intensity = int(255 * (1 - weight))  # æƒé‡è¶Šé«˜ï¼Œçº¢è‰²è¶Šæ·±
        bg_color = f"rgb({255}, {intensity}, {intensity})"
        html_content += f'<span class="token" style="background-color: {bg_color}" title="æƒé‡: {weight:.3f}">{token}</span>'
    
    html_content += """
    </div>
    
    <div class="info">
        <strong>ä½¿ç”¨è¯´æ˜:</strong><br>
        â€¢ é¼ æ ‡æ‚¬åœæŸ¥çœ‹å…·ä½“æƒé‡å€¼<br>
        â€¢ çº¢è‰²è¶Šæ·±è¡¨ç¤ºæ³¨æ„åŠ›æƒé‡è¶Šé«˜<br>
        â€¢ åˆ†è¯è¾¹ç•Œå¯èƒ½ä¸ç¬¦åˆäººç±»ç›´è§‰
    </div>
    
    <div class="tokenizer-info">
        <h3>ğŸ“ åˆ†è¯ç»“æœè¯¦æƒ…</h3>
        <div style="background: white; padding: 10px; border-radius: 4px; border: 1px solid #dee2e6;">
"""
    
    # æ·»åŠ åˆ†è¯è¯¦æƒ…
    for i, (token, weight) in enumerate(zip(tokens, normalized_weights)):
        escaped_token = token.replace('<', '&lt;').replace('>', '&gt;').replace('&', '&amp;')
        html_content += f'<span style="margin: 2px; padding: 2px 6px; background: #e9ecef; border-radius: 3px; font-family: monospace;">[{i}] "{escaped_token}" (æƒé‡: {weight:.3f})</span> '
    
    html_content += """
        </div>
        <div style="margin-top: 10px; font-size: 12px; color: #6c757d;">
            æ³¨ï¼šæ–¹æ‹¬å·å†…æ•°å­—ä¸ºtokenåœ¨åºåˆ—ä¸­çš„ä½ç½®ï¼Œå¼•å·å†…ä¸ºå®é™…çš„tokenæ–‡æœ¬
        </div>
    </div>
    
</body>
</html>
"""
    
    # ä¿å­˜HTMLæ–‡ä»¶
    output_path = "attention_viz.html"
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(html_content)
    
    print(f"æƒé‡å½¢çŠ¶: torch.Size([1, {len(tokens)}])")
    print(f"ç”ŸæˆHTMLæ–‡ä»¶: {output_path}")
    return output_path

if __name__ == "__main__":
    print("å¼€å§‹æ³¨æ„åŠ›å¯è§†åŒ–...")
    html_file = visualize_attention()
    print(f"å®Œæˆï¼è¯·æ‰“å¼€ {html_file} æŸ¥çœ‹å¯è§†åŒ–ç»“æœ")