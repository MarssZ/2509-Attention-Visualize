# æ³¨æ„åŠ›é¢„æµ‹é“¾æŠ€æœ¯éªŒè¯æŠ¥å‘Š

## æ ¸å¿ƒéªŒè¯ç»“æœ
**âœ… æŠ€æœ¯æ–¹æ¡ˆå®Œå…¨å¯è¡Œ**

## å…³é”®æŠ€æœ¯éªŒè¯

### 1. æ¨¡å‹è¾“å‡ºèƒ½åŠ›éªŒè¯
- **é—®é¢˜**: ç°æœ‰ä»£ç ä½¿ç”¨ `AutoModel`ï¼Œæ˜¯å¦æ”¯æŒtokené¢„æµ‹ï¼Ÿ
- **ç»“æœ**: âŒ `AutoModel` ä¸æä¾›logitsè¾“å‡º
- **è§£å†³æ–¹æ¡ˆ**: âœ… æ”¹ç”¨ `AutoModelForCausalLM` è·å–logits

### 2. æ ¸å¿ƒAPIéªŒè¯
- **API**: `AutoModelForCausalLM.from_pretrained()` + `output_attentions=True`
- **æµ‹è¯•ç»“æœ**: âœ… åŒæ—¶è·å¾—æ³¨æ„åŠ›æƒé‡å’Œé¢„æµ‹logits
- **è¾“å‡ºæ ¼å¼**:
  ```python
  outputs.attentions[-1]  # æ³¨æ„åŠ›æƒé‡ [1, heads, seq_len, seq_len]
  outputs.logits[0, -1, :] # æœ€åä½ç½®é¢„æµ‹ [vocab_size=151936]
  ```

### 3. é¢„æµ‹åŠŸèƒ½éªŒè¯
- **è¾“å…¥**: "æˆ‘çˆ±ä¸­å›½"
- **é¢„æµ‹token**: "çš„" (æ¦‚ç‡28.7%)
- **éªŒè¯**: âœ… é¢„æµ‹ç»“æœåˆç†ï¼Œæ¦‚ç‡åˆ†å¸ƒæ­£å¸¸

## å¿…éœ€ä¿®æ”¹

### å½“å‰ä»£ç é—®é¢˜
```python
# ç°æœ‰ä»£ç  - æ— æ³•è·å–logits
model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
```

### ä¿®å¤æ–¹æ¡ˆ
```python
# ä¿®æ”¹ä¸º - å¯è·å–logits
model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)

# åŒæ—¶è·å–æ³¨æ„åŠ›å’Œé¢„æµ‹
outputs = model(**inputs, output_attentions=True)
attention = outputs.attentions[-1]  # æ³¨æ„åŠ›æƒé‡
logits = outputs.logits[0, -1, :]   # é¢„æµ‹åˆ†æ•°
probs = torch.softmax(logits, dim=-1)  # æ¦‚ç‡åˆ†å¸ƒ
```

## é›†æˆå¤æ‚åº¦
**ğŸŸ¢ ç®€å•** - åªéœ€ä¿®æ”¹ä¸€è¡Œå¯¼å…¥å’Œæ·»åŠ å‡ è¡Œé¢„æµ‹ä»£ç 

## æ€§èƒ½å½±å“
**ğŸŸ¢ æœ€å°** - å¤ç”¨ç›¸åŒçš„æ¨¡å‹å‰å‘ä¼ æ’­ï¼Œæ— é¢å¤–è®¡ç®—å¼€é”€

## é£é™©è¯„ä¼°
**ğŸŸ¢ ä½é£é™©** - æˆç†Ÿçš„transformersåº“APIï¼Œå¹¿æ³›éªŒè¯

## ç«‹å³è¡ŒåŠ¨å»ºè®®
1. ä¿®æ”¹ `app.py` ä¸­çš„æ¨¡å‹åŠ è½½æ–¹å¼
2. åœ¨ `get_attention_visualization_data` å‡½æ•°ä¸­æ·»åŠ é¢„æµ‹é€»è¾‘
3. æ‰©å±•è¿”å›å€¼åŒ…å«é¢„æµ‹tokenå’Œæ¦‚ç‡
4. æ›´æ–°å‰ç«¯æ˜¾ç¤ºé€»è¾‘